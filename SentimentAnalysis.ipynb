{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e087faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#!pip install wordcloud\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1575f90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>@united sitting on the runway in Newark, they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@SouthwestAir the last 4 times I've arrived @L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>@united Thanks for the reminder. It's been a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@united when an airline causes the missed conn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@USAirways offloading the plane?!?!?! This is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0          negative  @united sitting on the runway in Newark, they ...\n",
       "1          negative  @SouthwestAir the last 4 times I've arrived @L...\n",
       "2          positive  @united Thanks for the reminder. It's been a f...\n",
       "3          negative  @united when an airline causes the missed conn...\n",
       "4          negative  @USAirways offloading the plane?!?!?! This is ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('AirlineTweets.csv')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data = data[['airline_sentiment','text']]\n",
    "print(data.shape)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d53274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "punctuation = '!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'\n",
    "\n",
    "def remove_handle(text, pattern):\n",
    "    r = re.findall(pattern,text)\n",
    "    for word in r:\n",
    "        text = re.sub(word, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "data['clean_text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['clean_text'] = np.vectorize(remove_handle)(data['clean_text'],'@[\\w]*')\n",
    "data['clean_text'] = data['clean_text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]', '', x))\n",
    "#data.head()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20fb3460",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2fd7473506ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Exploratory Data Analysis: Visualization of frequent words in tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m450\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_font_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Exploratory Data Analysis: Visualization of frequent words in tweets\n",
    "all_words = ' '.join([tweet for tweet in data['clean_text']])\n",
    "wordcloud = WordCloud(width=750, height=450, random_state=33, max_font_size=100).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb6dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis: visualization of frequent words in tweets labeled positive\n",
    "all_words = ' '.join([tweet for tweet in data['clean_text'][data['airline_sentiment']=='positive']])\n",
    "wordcloud = WordCloud(width=750, height=450, random_state=42, max_font_size=100).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis: visualization of frequent words in tweets labeled neutral\n",
    "all_words = ' '.join([tweet for tweet in data['clean_text'][data['airline_sentiment']=='neutral']])\n",
    "wordcloud = WordCloud(width=750, height=450, random_state=42, max_font_size=100).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8702918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis: visualization of frequent words in tweets labeled negative\n",
    "all_words = ' '.join([tweet for tweet in data['clean_text'][data['airline_sentiment']=='negative']])\n",
    "wordcloud = WordCloud(width=750, height=450, random_state=42, max_font_size=100).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b7b9695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization, Padding sequences\n",
    "max_words = 4500\n",
    "tokenizer = Tokenizer(num_words=max_words, split=' ')\n",
    "tokenizer.fit_on_texts(data['clean_text'].values) # creates an internal dictionary based on the tweets\n",
    "\n",
    "x = tokenizer.texts_to_sequences(data['clean_text'].values) # replaces words in the tweets with corresponding int values\n",
    "x = pad_sequences(x) # padding our text vector so they all have the same length\n",
    "#len(x)\n",
    "#print(x.shape)\n",
    "#print(x.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b397f2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 32, 256)           1152000   \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 32, 256)           0         \n",
      "                                                                 \n",
      " lstm_22 (LSTM)              (None, 32, 256)           525312    \n",
      "                                                                 \n",
      " lstm_23 (LSTM)              (None, 256)               525312    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,203,395\n",
      "Trainable params: 2,203,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/8\n",
      "321/321 - 168s - loss: 0.6713 - accuracy: 0.7190 - 168s/epoch - 523ms/step\n",
      "Epoch 2/8\n",
      "321/321 - 150s - loss: 0.4457 - accuracy: 0.8255 - 150s/epoch - 468ms/step\n",
      "Epoch 3/8\n",
      "321/321 - 149s - loss: 0.3512 - accuracy: 0.8653 - 149s/epoch - 465ms/step\n",
      "Epoch 4/8\n",
      "321/321 - 155s - loss: 0.2821 - accuracy: 0.8936 - 155s/epoch - 482ms/step\n",
      "Epoch 5/8\n",
      "321/321 - 161s - loss: 0.2279 - accuracy: 0.9181 - 161s/epoch - 503ms/step\n",
      "Epoch 6/8\n",
      "321/321 - 140s - loss: 0.1936 - accuracy: 0.9269 - 140s/epoch - 438ms/step\n",
      "Epoch 7/8\n",
      "321/321 - 176s - loss: 0.1604 - accuracy: 0.9419 - 176s/epoch - 549ms/step\n",
      "Epoch 8/8\n",
      "321/321 - 171s - loss: 0.1435 - accuracy: 0.9477 - 171s/epoch - 532ms/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "output_space_dim = 256\n",
    "model.add(Embedding(input_dim=max_words, output_dim=output_space_dim, input_length=x.shape[1])) # input_lenth=32=number of words in a tweet\n",
    "model.add(Dropout(rate=0.3)) # helps prevent overfitting, 30% input units are dropped (set to 0) during training time while the rest are scaled up so that the sum is unchanged\n",
    "model.add(LSTM(units=output_space_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))\n",
    "model.add(LSTM(units=output_space_dim, dropout=0.3, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=3, activation='softmax')) # densely connected NN layer\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "Y = pd.get_dummies(data['airline_sentiment']).values #converts categorical data .i.e. labels to dummy variables\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "\n",
    "RNNmodel = model.fit(x_train, y_train, epochs=8, verbose=2) # default batch size = 32\n",
    "\n",
    "model.save('sentiment_analysis.h5')\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "#[print(data['text'][i], predictions[i], y_test[i]) for i in range(0, 10)]\n",
    "\n",
    "# pos_count, neu_count, neg_count = 0, 0, 0\n",
    "# real_pos, real_neu, real_neg = 0, 0, 0\n",
    "# for i, prediction in enumerate(predictions):\n",
    "#     if np.argmax(prediction)==2:\n",
    "#         pos_count += 1\n",
    "#     elif np.argmax(prediction)==1:\n",
    "#         neu_count += 1\n",
    "#     else:\n",
    "#         neg_count += 1\n",
    "    \n",
    "#     if np.argmax(y_test[i])==2:\n",
    "#         real_pos += 1\n",
    "#     elif np.argmax(y_test[i])==1:    \n",
    "#         real_neu += 1\n",
    "#     else:\n",
    "#         real_neg +=1\n",
    "\n",
    "# print('Positive predictions:', pos_count)\n",
    "# print('Neutral predictions:', neu_count)\n",
    "# print('Negative predictions:', neg_count)\n",
    "# print('Real positive:', real_pos)\n",
    "# print('Real neutral:', real_neu)\n",
    "# print('Real negative:', real_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8af914d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RNNmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7fe3a235c439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RNNmodel' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(RNNmodel.history['accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a0027f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords done\n",
      "ah\n",
      "feature vectors!\n",
      "true sentiments!\n",
      "extraction done\n",
      "Most Informative Features\n",
      "         contains(kudos) = True           positi : negati =     63.2 : 1.0\n",
      "     contains(wonderful) = True           positi : negati =     50.3 : 1.0\n",
      "      contains(favorite) = True           positi : negati =     32.3 : 1.0\n",
      "      contains(passbook) = True           positi : negati =     32.3 : 1.0\n",
      "       contains(helpful) = True           positi : neutra =     30.0 : 1.0\n",
      "     contains(beautiful) = True           positi : negati =     29.7 : 1.0\n",
      "   contains(outstanding) = True           positi : negati =     29.7 : 1.0\n",
      "     contains(fantastic) = True           positi : negati =     27.1 : 1.0\n",
      "           contains(lt3) = True           positi : negati =     27.1 : 1.0\n",
      "        contains(prompt) = True           positi : negati =     27.1 : 1.0\n",
      "classified!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"int\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0684b701b35e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_sentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtrue_sentiments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtrue_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"int\") to tuple"
     ]
    }
   ],
   "source": [
    "# Naive-Bayes Classifier\n",
    "\n",
    "index = range(len(data['clean_text']))\n",
    "#data.drop('text', axis=1, inplace=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data,index,test_size=0.20,random_state=0)\n",
    "\n",
    "train_tweets=[]\n",
    "true_sentiments=[]        \n",
    "word_features=[]\n",
    "#stopwords = stopwords.words(\"english\")\n",
    "\n",
    "print(\"stopwords done\")\n",
    "\n",
    "def get_feature_vector(tweet):\n",
    "    feature_vector = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        #replace two or more occurences of a word with two occurrences\n",
    "        repetition = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "        word = repetition.sub(r\"\\1\\1\", word)\n",
    "        #check if the word begins with a letter or number\n",
    "        val = re.search(r\"^[a-z][a-z0-9]*$\", word)\n",
    "        #ignore if it is a stop word\n",
    "        #if(word in stopwords or val is None):\n",
    "        if(val is None):\n",
    "            continue\n",
    "        else:\n",
    "            feature_vector.append(word)\n",
    "    return feature_vector\n",
    "\n",
    "print(\"ah\")\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    sentiment = x_train['airline_sentiment'][y_train[i]]\n",
    "    tweet = x_train['clean_text'][y_train[i]]\n",
    "    feature_vector = get_feature_vector(tweet)\n",
    "    word_features.extend(feature_vector)\n",
    "    train_tweets.append((feature_vector, sentiment))\n",
    "\n",
    "    \n",
    "print(\"feature vectors!\")    \n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    sentiment = x_test['airline_sentiment'][y_test[i]]\n",
    "    true_sentiments.append((sentiment))\n",
    "    \n",
    "print(\"true sentiments!\")\n",
    "                             \n",
    "# Extracts each feature from the tweet and stores it in a dictionary            \n",
    "def extract_features(tweet):\n",
    "    words = set(tweet)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in words)\n",
    "    return features\n",
    "word_features = list(set(word_features))\n",
    "\n",
    "print(\"extraction done\")\n",
    "                             \n",
    "training = nltk.classify.util.apply_features(extract_features, train_tweets)\n",
    "classifier = nltk.NaiveBayesClassifier.train(training)\n",
    "classifier.show_most_informative_features()\n",
    "test = x_test['clean_text'].apply(lambda tweet:extract_features(get_feature_vector(tweet)))\n",
    "test_sentiment = test.apply(lambda x: classifier.classify(x))\n",
    "\n",
    "print(\"classified!\")\n",
    "\n",
    "# Finding overall accuracy and accuracy for positive, neutral, negative tweets each\n",
    "accuracy = 0,\n",
    "true_pos, true_neu, true_neg = 0,0,0\n",
    "pos_accuracy, neu_accuracy, neg_accuracy = 0,0,0\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    \n",
    "    if (test_sentiment[y_test[i]]==true_sentiments[i]):\n",
    "        accuracy += 1\n",
    "        if(test_sentiment[y_test[i]]=='positive'):\n",
    "            true_pos += 1\n",
    "        elif(test_sentiment[y_test[i]]=='neutral'):\n",
    "            true_neu += 1\n",
    "        else:\n",
    "            true_neg = true_neg + 1\n",
    "    \n",
    "    if (true_sentiments[i]=='positive'):\n",
    "        pos_accuracy += 1\n",
    "    elif (true_sentiments[i]=='neutral'):\n",
    "        neu_accuracy += 1\n",
    "    else:\n",
    "        neg_accuracy += 1         \n",
    "\n",
    "accuracy = accuracy/float(len(x_test))\n",
    "pos_precision = true_pos/float(pos_pre)\n",
    "neu_precision = true_neu/float(neu_pre)\n",
    "neg_precision = true_neg/float(neg_pre)\n",
    "\n",
    "print (\"Accuracy is \" + str(accuracy))\n",
    "print (\"Positive precision is \" + str(pos_precision))\n",
    "print (\"Neutral precision is \" + str(neu_precision))\n",
    "print (\"Negative precision is \" + str(neg_precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c234e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
